"""Read X for llm context"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_read.ipynb.

# %% auto 0
__all__ = ['read_url', 'read_gist', 'read_gh_file', 'read_file', 'is_unicode', 'read_dir', 'read_pdf', 'read_yt_transcript',
           'read_google_sheet', 'gdoc_url_to_parseable', 'read_gdoc']

# %% ../nbs/00_read.ipynb 8
import httpx 
import html2text
from fastcore.all import delegates, ifnone

import re, os, glob, string
import requests
import fnmatch, mimetypes

from PyPDF2 import PdfReader
from toolslm.download import html2md, read_html

# %% ../nbs/00_read.ipynb 12
def read_url(url, # url to read
             heavy=False, # Use contactless browser
             sel=None,
             **kwargs): 
    "Reads a url and converts to markdown"
    if heavy: 
        from playwrightnb import url2md
        url2md(url,sel=ifnone(sel,'body'), **kwargs)
    return read_html(url,**kwargs)

# %% ../nbs/00_read.ipynb 17
def read_gist(url):
    "Returns raw gist content, or None"
    pattern = r'https://gist\.github\.com/([^/]+)/([^/]+)'
    match = re.match(pattern, url)
    if match:
        user, gist_id = match.groups()
        raw_url = f'https://gist.githubusercontent.com/{user}/{gist_id}/raw'
        return httpx.get(raw_url).text
    else:
        return None

# %% ../nbs/00_read.ipynb 24
def read_gh_file(url):
    pattern = r'https://github\.com/([^/]+)/([^/]+)/blob/([^/]+)/(.+)'
    replacement = r'https://raw.githubusercontent.com/\1/\2/refs/heads/\3/\4'
    raw_url = re.sub(pattern, replacement, url)
    return httpx.get(raw_url).text

# %% ../nbs/00_read.ipynb 28
def read_file(path):
    return open(path,'r').read()

# %% ../nbs/00_read.ipynb 31
def is_unicode(filepath, sample_size=1024):
    try:
        with open(filepath, 'r') as file:
            sample = file.read(sample_size)
        return True
    except UnicodeDecodeError:
        return False

# %% ../nbs/00_read.ipynb 34
def read_dir(path, 
             exclude_non_unicode=True,
             excluded_patterns=[".git/**"],
             verbose=True):
    pattern = '**/*'
    result = []
    for file_path in glob.glob(os.path.join(path, pattern), recursive=True):
        if any(fnmatch.fnmatch(file_path, pat) for pat in excluded_patterns):
            continue
        if os.path.isfile(file_path):
            if exclude_non_unicode and not is_unicode(file_path):
                continue
            if verbose:
                print(f"Including {file_path}")
            result.append(f"--- File: {file_path} ---")
            with open(file_path, 'r', errors='ignore') as f:
                result.append(f.read())
            result.append(f"--- End of {file_path} ---")
    return '\n'.join(result)

# %% ../nbs/00_read.ipynb 37
def read_pdf(file_path: str) -> str:
    with open(file_path, 'rb') as file:
        reader = PdfReader(file)
        return ' '.join(page.extract_text() for page in reader.pages)

# %% ../nbs/00_read.ipynb 40
def read_yt_transcript(yt_url):
    from pytube import YouTube
    from youtube_transcript_api import YouTubeTranscriptApi
    try:
        yt = YouTube(yt_url)
        video_id = yt.video_id
    except Exception as e:
        print(f"An error occurred parsing yt urul: {e}")
        return None
    transcript = YouTubeTranscriptApi.get_transcript(video_id)
    return ' '.join(entry['text'] for entry in transcript) 

# %% ../nbs/00_read.ipynb 43
def read_google_sheet(orig_url):
    sheet_id = orig_url.split('/d/')[1].split('/')[0]
    csv_url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&id={sheet_id}&gid=0'
    res = requests.get(url=csv_url)
    return res.content

# %% ../nbs/00_read.ipynb 46
def gdoc_url_to_parseable(url):
    pattern = r'(https://docs\.google\.com/document/d/[^/]+)/edit'
    replacement = r'\1/export?format=html'
    return re.sub(pattern, replacement, url)

# %% ../nbs/00_read.ipynb 48
def read_gdoc(url):
    import html2text
    doc_url = url
    doc_id = doc_url.split('/d/')[1].split('/')[0]
    export_url = f'https://docs.google.com/document/d/{doc_id}/export?format=html'
    html_doc_content = requests.get(export_url).text
    doc_content = html2text.html2text(html_doc_content)
    return doc_content
